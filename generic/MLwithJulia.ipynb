{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation of the Iris dataset in Julia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before getting to the real Machine Learning part, it is necessary to get the data imported and prepared. I will cover only three basic steps here: importing a csv file, one hot encoding a categorical variable, and making a train-test split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep 1 — Import a CSV file in Julia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step to getting started in Julia is to import data. In this case, we use a csv file with the Iris data. For importing a csv file as a Data Frame, you will need to add the libraries “CSV” and “DataFrames” as shown below. Then, you use the “CSV.File” function to read the csv file and the DataFrame function to convert it to a data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Updating\u001b[22m\u001b[39m registry at `~/.julia/registries/General`\n",
      "######################################################################### 100,0%\n",
      "\u001b[32m\u001b[1m  Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m DataValueInterfaces ───────── v1.0.0\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m InvertedIndices ───────────── v1.0.0\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m DataAPI ───────────────────── v1.3.0\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m Missings ──────────────────── v0.4.4\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m IteratorInterfaceExtensions ─ v1.0.0\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m Tables ────────────────────── v1.1.0\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m DataFrames ────────────────── v0.21.8\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m SentinelArrays ────────────── v1.2.16\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m TableTraits ───────────────── v1.0.0\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m PooledArrays ──────────────── v0.5.3\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m StructTypes ───────────────── v1.1.0\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m CategoricalArrays ─────────── v0.8.3\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m SortingAlgorithms ─────────── v0.3.1\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m CSV ───────────────────────── v0.7.7\n",
      "\u001b[32m\u001b[1mUpdating\u001b[22m\u001b[39m `~/.julia/environments/v1.5/Project.toml`\n",
      " \u001b[90m [336ed68f] \u001b[39m\u001b[92m+ CSV v0.7.7\u001b[39m\n",
      "\u001b[32m\u001b[1mUpdating\u001b[22m\u001b[39m `~/.julia/environments/v1.5/Manifest.toml`\n",
      " \u001b[90m [336ed68f] \u001b[39m\u001b[92m+ CSV v0.7.7\u001b[39m\n",
      " \u001b[90m [324d7699] \u001b[39m\u001b[92m+ CategoricalArrays v0.8.3\u001b[39m\n",
      " \u001b[90m [9a962f9c] \u001b[39m\u001b[92m+ DataAPI v1.3.0\u001b[39m\n",
      " \u001b[90m [a93c6f00] \u001b[39m\u001b[92m+ DataFrames v0.21.8\u001b[39m\n",
      " \u001b[90m [e2d170a0] \u001b[39m\u001b[92m+ DataValueInterfaces v1.0.0\u001b[39m\n",
      " \u001b[90m [41ab1584] \u001b[39m\u001b[92m+ InvertedIndices v1.0.0\u001b[39m\n",
      " \u001b[90m [82899510] \u001b[39m\u001b[92m+ IteratorInterfaceExtensions v1.0.0\u001b[39m\n",
      " \u001b[90m [e1d29d7a] \u001b[39m\u001b[92m+ Missings v0.4.4\u001b[39m\n",
      " \u001b[90m [2dfb63ee] \u001b[39m\u001b[92m+ PooledArrays v0.5.3\u001b[39m\n",
      " \u001b[90m [91c51154] \u001b[39m\u001b[92m+ SentinelArrays v1.2.16\u001b[39m\n",
      " \u001b[90m [a2af1166] \u001b[39m\u001b[92m+ SortingAlgorithms v0.3.1\u001b[39m\n",
      " \u001b[90m [856f2bd8] \u001b[39m\u001b[92m+ StructTypes v1.1.0\u001b[39m\n",
      " \u001b[90m [3783bdb8] \u001b[39m\u001b[92m+ TableTraits v1.0.0\u001b[39m\n",
      " \u001b[90m [bd369af6] \u001b[39m\u001b[92m+ Tables v1.1.0\u001b[39m\n",
      " \u001b[90m [9fa8497b] \u001b[39m\u001b[92m+ Future\u001b[39m\n",
      "\u001b[32m\u001b[1m  Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1mUpdating\u001b[22m\u001b[39m `~/.julia/environments/v1.5/Project.toml`\n",
      " \u001b[90m [a93c6f00] \u001b[39m\u001b[92m+ DataFrames v0.21.8\u001b[39m\n",
      "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.5/Manifest.toml`\n",
      "┌ Info: Precompiling CSV [336ed68f-0bac-5ca0-87d4-7b16caf5d00b]\n",
      "└ @ Base loading.jl:1278\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "ArgumentError: \"mypath//iris.csv\" is not a valid file",
     "output_type": "error",
     "traceback": [
      "ArgumentError: \"mypath//iris.csv\" is not a valid file",
      "",
      "Stacktrace:",
      " [1] CSV.Header(::String, ::Int64, ::Bool, ::Int64, ::Nothing, ::Int64, ::Bool, ::Nothing, ::Nothing, ::Bool, ::Nothing, ::Nothing, ::Array{String,1}, ::String, ::Nothing, ::Bool, ::Char, ::Nothing, ::Nothing, ::Char, ::Nothing, ::Nothing, ::UInt8, ::Array{String,1}, ::Array{String,1}, ::Nothing, ::Nothing, ::Dict{Type,Type}, ::Nothing, ::Float64, ::Bool, ::Bool, ::Bool, ::Bool, ::Bool, ::Bool) at /home/lukas/.julia/packages/CSV/MKemC/src/header.jl:92",
      " [2] CSV.File(::String; header::Int64, normalizenames::Bool, datarow::Int64, skipto::Nothing, footerskip::Int64, transpose::Bool, comment::Nothing, use_mmap::Nothing, ignoreemptylines::Bool, select::Nothing, drop::Nothing, missingstrings::Array{String,1}, missingstring::String, delim::Nothing, ignorerepeated::Bool, quotechar::Char, openquotechar::Nothing, closequotechar::Nothing, escapechar::Char, dateformat::Nothing, dateformats::Nothing, decimal::UInt8, truestrings::Array{String,1}, falsestrings::Array{String,1}, type::Nothing, types::Nothing, typemap::Dict{Type,Type}, categorical::Nothing, pool::Float64, lazystrings::Bool, strict::Bool, silencewarnings::Bool, debug::Bool, parsingdebug::Bool, kw::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/lukas/.julia/packages/CSV/MKemC/src/file.jl:216",
      " [3] CSV.File(::String) at /home/lukas/.julia/packages/CSV/MKemC/src/file.jl:216",
      " [4] top-level scope at In[1]:5",
      " [5] include_string(::Function, ::Module, ::String, ::String) at ./loading.jl:1091",
      " [6] execute_code(::String, ::String) at /home/lukas/.julia/packages/IJulia/rWZ9e/src/execute_request.jl:27",
      " [7] execute_request(::ZMQ.Socket, ::IJulia.Msg) at /home/lukas/.julia/packages/IJulia/rWZ9e/src/execute_request.jl:86",
      " [8] #invokelatest#1 at ./essentials.jl:710 [inlined]",
      " [9] invokelatest at ./essentials.jl:709 [inlined]",
      " [10] eventloop(::ZMQ.Socket) at /home/lukas/.julia/packages/IJulia/rWZ9e/src/eventloop.jl:8",
      " [11] (::IJulia.var\"#15#18\")() at ./task.jl:356"
     ]
    }
   ],
   "source": [
    "# import a csv file\n",
    "import Pkg; Pkg.add(\"CSV\")\n",
    "import Pkg; Pkg.add(\"DataFrames\")\n",
    "using CSV, DataFrames\n",
    "iris = DataFrame(CSV.File(\"mypath//iris.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep 2 — One Hot Encode the dependent variable (variety)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some models, you will need one hot encoding for the categorical variables. You can use the “Lathe” library for this. It has a OneHotEncode function that will convert the data frame into a OneHotEncoded data frame. After that, you can remove the original column using the “select” function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Pkg; Pkg.add(\"Lathe\")\n",
    "using Lathe\n",
    "scaled_feature = Lathe.preprocess.OneHotEncode(iris,:variety)\n",
    "iris = select!(iris, Not([:variety]))\n",
    "first(iris,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep 3 — Train Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For model evaluation, you will need a train test split. The following code does this using the library “Random”. Basically, it selects a random subset of indexes and treats these as the train set, while the non-selected indexes will be the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Random\n",
    "sample = randsubseq(1:size(iris,1), 0.75)\n",
    "train = iris[sample, :]\n",
    "notsample = [i for i in 1:size(iris,1) if isempty(searchsorted(sample, i))]\n",
    "test = iris[notsample, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning in Julia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resources for Machine Learning in Julia are still relatively distributed over different packages. Julia being not (yet) as popular as other programming languages for Machine Learning, it can sometimes be a bit of work to find specific models. It can also be more effort to find (or write) certain basic data preparation functions that are easily available in Python and R.\n",
    "\n",
    "The good news is that there are initiatives to regroup Machine Learning models in larger libraries. At this point, there are two libraries that are seriously competing for becoming the go-to Machine Learning library in Julia: MLJ and Scikit Learn.\n",
    "\n",
    "Those two initiatives are great, but they are not yet totally complete. As a result, for some models, they simply provide wrappers to other, much smaller, Machine Learning libraries. Because of this, I find it important to also cover two of those smaller libraries: “GLM” for Generalized Linear Models and “DecisionTree” for many tree-based models. I will start with the smaller libraries and finish with the larger initiatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression in Julia using the GLM library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example fits three Logistic Regression models using the GLM library on the Iris data. GLM uses the “formula” interface, which is common in statistics-oriented libraries. We can specify a family (Binomial in this case) and a link type (Logit Link in this case) in order to create the type of GLM that is desired. This is done in the first part of the below code snippet.\n",
    "\n",
    "At the end of this snippet, the predictions of the three models are horizontally concatenated in order to prepare for the application of a One-Versus-All multi-class classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Pkg; Pkg.add(\"StatsModels\")\n",
    "import Pkg; Pkg.add(\"GLM\")\n",
    "\n",
    "using DataFrames, GLM\n",
    "fm_setosa = @formula(Setosa ~  sepallength + sepalwidth + petallength + petalwidth)\n",
    "lm_setosa = glm(fm_setosa, train, Binomial(), LogitLink())\n",
    "pred_setosa = predict(lm_setosa, test)\n",
    "\n",
    "fm_virginica = @formula(Virginica ~ sepallength + sepalwidth + petallength + petalwidth)\n",
    "lm_virginica = glm(fm_virginica, train, Binomial(), LogitLink())\n",
    "pred_virginica = predict(lm_virginica, test)\n",
    "\n",
    "fm_versicolor = @formula(Versicolor ~ sepallength + sepalwidth + petallength + petalwidth)\n",
    "lm_versicolor = glm(fm_versicolor, train, Binomial(), LogitLink())\n",
    "pred_versicolor = predict(lm_versicolor, test)\n",
    "\n",
    "preds = hcat(pred_setosa, pred_virginica, pred_versicolor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following snippet, we convert the three predicted probabilities for each row into one class prediction per row. This decision is based on the highest predicted probability between each of the three predicted probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reclass by maximum predicted probability\n",
    "preds_cat = String[];\n",
    "for i in 1:nrow(DataFrame(preds))\n",
    "    if pred_setosa[i] >= pred_virginica[i] && pred_setosa[i] >= pred_versicolor[i]\n",
    "        preds_cat = vcat(preds_cat ,\"Setosa\")\n",
    "    elseif pred_versicolor[i] >= pred_virginica[i] && pred_versicolor[i] >= pred_setosa[i]\n",
    "        preds_cat = vcat(preds_cat ,\"Versicolor\")\n",
    "    elseif pred_virginica[i] >= pred_versicolor[i] && pred_virginica[i] >= pred_setosa[i]\n",
    "        preds_cat = vcat(preds_cat ,\"Virginica\")\n",
    "    end\n",
    "end\n",
    "\n",
    "preds_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final step, here is how to compute the accuracy of our GLM prediction on the test set, using a short for-loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Accuracy of GLM\n",
    "\n",
    "correct = 0\n",
    "actual = orig_col[notsample]\n",
    "n=length(actual)\n",
    "for i in 1:n\n",
    "    if actual[i] == preds_cat[i]\n",
    "        correct = correct + 1\n",
    "    end\n",
    "end\n",
    "println(correct / n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree in Julia using the DecisionTree.jl library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code snippet, you will see how to fit a Decision Tree in Julia. Firstly, it re-imports the Iris data, because the Decision Tree supports the use of Categorical variables. As stated in the introduction, this is an advantage of using Julia.\n",
    "\n",
    "Then the model is created as an instantiation of the DecisionTreeClassifier. We can give several hyper parameters, as for example max_depth used in this example. The fit syntax is quite special with the exclamation mark.\n",
    "\n",
    "The last two steps are prediction on the test set using the predict function and computing the accuracy, as in the previous model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Pkg; Pkg.add(\"DecisionTree\")\n",
    "\n",
    "# re import iris, because RandomForests will handle the categorical variable\n",
    "iris = DataFrame(CSV.File(\"C://Users//jkorstan//Desktop//iris.csv\"))\n",
    "\n",
    "train = iris[sample, :]\n",
    "notsample = [i for i in 1:size(iris,1) if isempty(searchsorted(sample, i))]\n",
    "test = iris[notsample, :]\n",
    "            \n",
    "X_train = convert(Array, train[:, 1:4]);\n",
    "y_train = convert(Array, train[:, 5]);\n",
    "            \n",
    "X_test = convert(Array, test[:, 1:4]);\n",
    "y_test = convert(Array, test[:, 5]);\n",
    "\n",
    "      \n",
    "# Fit the model\n",
    "using DecisionTree\n",
    "\n",
    "model = DecisionTreeClassifier(max_depth=5)\n",
    "fit!(model, X_train, y_train)\n",
    "\n",
    "\n",
    "# Predict\n",
    "dectree_pred = DecisionTree.predict(model, X_test)\n",
    "\n",
    "                                    \n",
    "# Compute accuracy\n",
    "correct = 0\n",
    "n=length(y_test)\n",
    "for i in 1:n\n",
    "    if actual[i] == dectree_pred[i]\n",
    "        correct = correct + 1\n",
    "    end\n",
    "end\n",
    "println(correct / n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest in Julia using the DecisionTree.jl library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you will see, the Random Forest Model is applied in almost the same way as the Decision Tree. It may be confusing at first, but the Random Forest model is also part of the Decision Tree library!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using DecisionTree\n",
    "\n",
    "# Fit the model\n",
    "rf = RandomForestClassifier()\n",
    "fit!(rf, X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "rf_pred = DecisionTree.predict(rf, X_test)\n",
    "\n",
    "# Compute the accuracy\n",
    "correct = 0\n",
    "n=length(y_test)\n",
    "for i in 1:n\n",
    "    if actual[i] == rf_pred[i]\n",
    "        correct = correct + 1\n",
    "    end\n",
    "end\n",
    "println(correct / n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main packages for Machine Learning in Julia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have seen how to use two great, but small, libraries for Machine Learning in Julia, let’s get to the larger libraries. As stated before, there are two main packages that compete for becoming the go-to ML library in Julia: Scikit Learn and MLJ. Let’s check them both out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit Learn for Machine Learning in Julia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of you will know Scikit Learn from Python. It is the package for Machine Learning in Python, and it is great to have it in Julia as well. It requires much less effort if we can just use the same syntax as Python!\n",
    "\n",
    "Let’s see an example of Scikit Learn in Julia. This code snippet starts with importing the Scikit Learn library. The next step is loading the model you want to use (in this case a Logistic Regression). Using the “fit!” syntax (attention to the exclamation mark), the model is trained.\n",
    "\n",
    "After that, the predict function is used to predict the test set with the trained model. Finally, the accuracy is computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the library\n",
    "import Pkg; Pkg.add(\"ScikitLearn\")\n",
    "\n",
    "# Import the model you want to use\n",
    "using ScikitLearn\n",
    "@sk_import linear_model: LogisticRegression\n",
    "\n",
    "# Fit the model\n",
    "log_reg = fit!(LogisticRegression(), X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "sklearn_pred = log_reg.predict(X_test)\n",
    "\n",
    "# Compute the accuracy\n",
    "correct = 0\n",
    "n=length(y_test)\n",
    "for i in 1:n\n",
    "    if y_test[i] == sklearn_pred[i]       \n",
    "        correct = correct + 1\n",
    "    end\n",
    "end\n",
    "println(correct / n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Scikit Learn in Julia also has its disadvantages. For example, a large part of the Scikit Learn library that we can use in Julia is actually just a wrapper around Python. Apart from a few models that have been implemented in Julia, the Julia models actually use Pycall to call Python code.\n",
    "\n",
    "However, if we want to switch to Julia, it should be for having the benefits of Julia. One of the main benefits of Julia being a speed advantage over Python, calling Python code is really not what we should be doing here. If it’s just a Python wrapper, we might as well stay with Scikit Learn in Python directly.\n",
    "\n",
    "Another disadvantage is that the Python models in Scikit Learn have no support for categorical variables. Apart from encoding them, there is really not much that can be done in Scikit Learn and that really is a negative point (especially for tree-based models). As you have seen throughout the examples, Julia allows us to treat a categorical variable as one variable, rather than as a set of one hot encoded dummies. So that advantage of Julia would also go away when we use Julia as a mere Python wrapper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLJ for Machine Learning in Julia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A competitor for Machine Learning in Julia is the MLJ package. It promises to solve the problem of categorical variables and it is pure-Julia. This makes it very interesting to explore. It also has serious support from the Alan Turing Foundation, which makes me believe that this library could be here to stay.\n",
    "\n",
    "Let’s now see an example of MLJ for Machine Learning in Julia in the below snippet. There are a few things that are different from usual. Especially, the creation of a machine is a choice of syntax that will be new for many. Then what’s unusual more is the fact to have to load a model, rather than importing a package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the packages that you need (this depends on the model you use)\n",
    "import Pkg; Pkg.add(\"MLJ\")\n",
    "import Pkg; Pkg.add(\"LIBSVM\")\n",
    "import Pkg; Pkg.add(\"MLJModels\")\n",
    "\n",
    "# load the model\n",
    "using MLJ\n",
    "svc_model = @load SVC verbosity=1\n",
    "\n",
    "# create a so-called machine\n",
    "svc = machine(svc_model, X_train, categorical(y_train))\n",
    "\n",
    "# fit the model\n",
    "MLJ.fit!(svc);\n",
    "\n",
    "# predict on the test set\n",
    "yhat = MLJ.predict(svc, X_test);\n",
    "\n",
    "#compute the accuracy\n",
    "correct = 0\n",
    "n=length(y_test)\n",
    "for i in 1:n\n",
    "    if actual[i] == yhat[i]       \n",
    "        correct = correct + 1\n",
    "    end\n",
    "end\n",
    "println(correct / n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But after those syntactical differences, the use of the MLJ library is not fundamentally different. MLJ syntax is easy to learn and there is good documentation on MLJ’s documentation website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this article, we have seen four libraries for Machine Learning in Julia. Two of those libraries (MLJ and Scikit Learn) seem to be real competitors to take over the Machine Learning landscape in Julia.\n",
    "\n",
    "Scikit Learn has the big advantage of the familiar syntax from the Python implementation and it has trust from its community. On the other hand, Scikit Learn is often simply calling Python code, which takes away most of the advantages of using Julia in the first place.\n",
    "\n",
    "MLJ has the big advantage of being a real Julia project. Its syntax is slightly new, but the differences seem minor. The real challenge for MLJ would be gaining trust and popularity by a larger community.\n",
    "\n",
    "I hope this article has given you all that you need for getting started in Julia and I wish you good luck doing so!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.2",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
